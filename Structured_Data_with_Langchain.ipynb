{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "1ydpVBRwQbWNpbR2RVpFC6NWJjf0Wl8h-",
      "authorship_tag": "ABX9TyONTDVd2BK1BINCkvqNtQs0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fsommers/ICMR24/blob/main/Structured_Data_with_Langchain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IC4L_A9A4ACr"
      },
      "outputs": [],
      "source": [
        "!pip install langchain\n",
        "!pip install langchain-openai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import base64\n",
        "import langchain\n",
        "from langchain_core.pydantic_v1 import BaseModel, Field\n",
        "from langchain_core.messages import HumanMessage\n",
        "from langchain_core.output_parsers import JsonOutputParser\n",
        "from langchain.chains import TransformChain\n",
        "from langchain_core.runnables import chain\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "from google.colab import userdata\n",
        "OPENAI_API_KEY=userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "class DocumentItem(BaseModel):\n",
        "  \"\"\"Structured information extracted from an image\"\"\"\n",
        "  customer_name: str = Field(\"The name of the customer\")\n",
        "  address: str = Field(\"The address of the customer\")\n",
        "  vin: str = Field(\"The Vehicle Identification Number (VIN)\")\n",
        "  car: str = Field(\"The car make and model\")\n",
        "\n",
        "auto_contract_prompt = \"\"\"\n",
        "  You are an expert at information extraction from images of automobile loan contracts.\n",
        "\n",
        "  Given this page of an automobile loan contract, extract the following information:\n",
        "    - The name of the customer\n",
        "    - The address of the customer\n",
        "    - The Vehicle Identification Number (VIN)\n",
        "    - The car make and model\n",
        "\n",
        "    Do not guess. If some information is missing just return \"N/A\" in the relevant field.\n",
        "    If you determine that the image is not of an automobile loan contract, just set all the fields in the formatting instructions to \"N/A\".\n",
        "\n",
        "    You must obey the output format under all circumstances. Please follow the formatting instructions exactly.\n",
        "    Do not return any additional comments or explanation.\n",
        "\"\"\"\n",
        "\n",
        "def load_image(inputs: dict) -> dict:\n",
        "  \"\"\"Load the image from a file and encode it as base64\"\"\"\n",
        "  image_path = inputs[\"image_path\"]\n",
        "\n",
        "  def encode_image(image_path):\n",
        "    with open(image_path, \"rb\") as image_file:\n",
        "      return base64.b64encode(image_file.read()).decode('utf-8')\n",
        "\n",
        "  image_base64 = encode_image(image_path)\n",
        "  return {\"image\": image_base64}\n",
        "\n",
        "parser = JsonOutputParser(pydantic_object=DocumentItem)\n",
        "\n",
        "@chain\n",
        "def doc_model(inputs: dict) -> str | list[str] | dict:\n",
        "  \"\"\"Invoke the model with an image and prompt\"\"\"\n",
        "  model = ChatOpenAI(temperature=0.0, model=\"gpt-4-turbo\", max_tokens=1024, api_key=OPENAI_API_KEY)\n",
        "  msg = model.invoke(\n",
        "      [HumanMessage(\n",
        "          content=[\n",
        "              {\"type\": \"text\", \"text\": inputs[\"prompt\"]},\n",
        "              {\"type\": \"text\", \"text\": parser.get_format_instructions()},\n",
        "              {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64, {inputs['image']}\"}}\n",
        "          ]\n",
        "      )]\n",
        "  )\n",
        "  return msg.content\n",
        "\n",
        "doc_transform = TransformChain(\n",
        "    input_variables=[\"image_path\"],\n",
        "    output_variables=[\"image\"],\n",
        "    transform=load_image\n",
        ")\n",
        "\n",
        "def get_document_information(image_path: str) -> dict:\n",
        "  doc_chain = doc_transform | doc_model | parser\n",
        "  return doc_chain.invoke({'image_path': f'{image_path}',\n",
        "                               'prompt': auto_contract_prompt})\n",
        "\n",
        "\n",
        "file = \"/content/drive/MyDrive/DOC_EXAMPLES/UNSEEN/CACONTRACT/2-JvUQx2cEWl3Oqu_M9ni.jpg\"\n",
        "info = get_document_information(file)\n",
        "print(info)"
      ],
      "metadata": {
        "id": "ujYU9RVo4_gV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}